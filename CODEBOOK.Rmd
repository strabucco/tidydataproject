---
title: "CODEBOOK"
author: "STrabucco"
date: "December 26, 2015"
output: html_document
---

This CODEBOOK describes the variables, data and transformations required to clean up the data from the following source: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones 

Variables: 
*Movement_name: type of movement being recorded, transformed from numbers to descriptive identifiers. Original variables associated with this:
1 WALKING
2 WALKING_UPSTAIRS
3 WALKING_DOWNSTAIRS
4 SITTING
5 STANDING
6 LAYING
*Subject_ID: number associated with each individual volunteer. 1-30
*t: refers to time signals
*f: refers to frequency signals in which Fast Fourier Transform was applied
*Body: refers to body acceleration signal (see below for description of original analysis performed by original group measuring data)
*Gravity: refers to gravity acceleration signal
*Accelerometer: data from 3-axial accelerometer (x, y, z) units: g
*Gyroscope: data from 3-axial gyroscope(x, y, z) units: radians/second
*X, Y, Z: refers to axis of measurement
*mean: calculated mean value
*std: calculated standard deviation
*Jerk: Body linear acceleration or angular velocity in time calculation
*Magnitude: magnitude of three dimensional signals calculated using euclidean norm
*Angle: angle between two vectors, specified using "by" to identify the two vector used.


Data: 
The data for analysis was downloaded from the provided URL. The description of the data from the README file provided by the original authors includes the following: 

"The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. 

The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. See 'features_info.txt' for more details."
Author information: Jorge L. Reyes-Ortiz, Davide Anguita, Alessandro Ghio, Luca Oneto.
Smartlab - Non Linear Complex Systems Laboratory
DITEN - Universitâ€¡ degli Studi di Genova.
Via Opera Pia 11A, I-16145, Genoa, Italy.
activityrecognition@smartlab.ws
www.smartlab.ws


Transformations:
In order to comply with tidy data principles the following transformations were applied to the data:
*a header row was added at the top of the file to identify the variable in each column
*movement ID numbers were converted into human readable format by using descriptive names
*variable names were converted into more easy to read format by eliminating most abbreviations and elaborating on the variable names in this CODEBOOK
*data for each subject was consolidated by calculating the mean for each movement type for each subject in order to make a more manageable set of data.

